# Elasticsearch Document Search MCP Server

A Model Context Protocol (MCP) server that provides hybrid search capabilities for documents with text and image content stored in Elasticsearch.

## Features

- **Hybrid Search**: Combines semantic search, keyword matching, and vector search for optimal results
- **Text Search**: Semantic understanding of document content using Elasticsearch's `semantic_text` field
- **Image Search**: Vector similarity search on image descriptions using Elasticsearch-native embeddings
- **Structured Results**: Returns document text excerpts along with relevant images and their descriptions
- **Elasticsearch-Native**: All embeddings generated by Elasticsearch inference API for consistency and performance

## Architecture

The search strategy uses three complementary approaches:

1. **Semantic Search**: Uses Elasticsearch's semantic_text field on `main_text` for contextual understanding
2. **Keyword Search**: Traditional match queries on document titles (boosted for relevance)
3. **Vector Search**: kNN search on image description embeddings using Elasticsearch inference API (nested query with query_vector_builder)

## Prerequisites

- Python 3.8+
- Elasticsearch 8.0+ with:
  - An index matching the expected mapping structure
  - API key authentication enabled
  - Inference API configured with an embedding model endpoint
  - The inference endpoint must output 384-dimensional vectors (or match your mapping)

## Installation

1. Clone or navigate to this directory:
```bash
cd document_search_tool
```

2. Install dependencies:
```bash
pip install -r requirements.txt
```

3. Create a `.env` file from the example:
```bash
cp .env.example .env
```

4. Edit `.env` with your Elasticsearch configuration:
```env
ELASTIC_URL=https://your-elasticsearch-instance.com:9200
ELASTIC_API_KEY=your_api_key_here
ELASTIC_INDEX=documents
INFERENCE_ID=my-embedding-model
```

The `INFERENCE_ID` should match the inference endpoint ID configured in your Elasticsearch instance.

## Elasticsearch Index Mapping

Your Elasticsearch index should have the following mapping structure:

```json
{
  "mappings": {
    "properties": {
      "title": {
        "type": "text",
        "fields": {
          "keyword": {
            "type": "keyword"
          }
        }
      },
      "filename": {
        "type": "keyword"
      },
      "main_text": {
        "type": "semantic_text",
        "inference_id": "your_inference_id"
      },
      "page_descriptions": {
        "type": "nested",
        "properties": {
          "page_number": {
            "type": "integer"
          },
          "description_text": {
            "type": "text"
          },
          "description_vector": {
            "type": "dense_vector",
            "dims": 384,
            "index": true,
            "similarity": "cosine"
          },
          "image_path": {
            "type": "keyword"
          },
          "image_dimensions": {
            "type": "object",
            "properties": {
              "width": {"type": "integer"},
              "height": {"type": "integer"}
            }
          }
        }
      },
      "extracted_date": {
        "type": "date"
      },
      "total_pages": {
        "type": "integer"
      },
      "output_directory": {
        "type": "keyword"
      }
    }
  }
}
```

## Running the Server

### Standalone Testing

You can test the server directly:

```bash
python server.py
```

### Using with Claude Desktop

Add to your Claude Desktop configuration file:

**MacOS**: `~/Library/Application Support/Claude/claude_desktop_config.json`

**Windows**: `%APPDATA%/Claude/claude_desktop_config.json`

```json
{
  "mcpServers": {
    "elastic-document-search": {
      "command": "python",
      "args": ["/absolute/path/to/document_search_tool/server.py"],
      "env": {
        "ELASTIC_URL": "https://your-elasticsearch-instance.com:9200",
        "ELASTIC_API_KEY": "your_api_key_here",
        "ELASTIC_INDEX": "documents",
        "INFERENCE_ID": "my-embedding-model"
      }
    }
  }
}
```

Alternatively, the server will read from the `.env` file if environment variables are not set.

## Usage

Once connected, you can use the `search_documents` tool to search your document index:

### Example Queries

```
Search for documents about "machine learning algorithms"
```

```
Find images related to "neural network architecture diagrams"
```

```
What documents discuss "data preprocessing techniques"?
```

### Tool Parameters

- **question** (required): The search query or question
- **top_k** (optional): Number of results to return (default: 10)
- **min_score** (optional): Minimum relevance score threshold (default: 0.5)

### Response Format

The tool returns structured results including:

- Document title and filename
- Relevance score
- Text excerpt from the document
- Relevant images with:
  - Page number
  - Description text
  - Image file path
  - Image dimensions
  - Relevance score

## Elasticsearch Inference Configuration

This MCP server uses **Elasticsearch-native embedding generation** via the Inference API. All embeddings are generated on the Elasticsearch side, ensuring consistency between indexing and querying.

### Setting Up Your Inference Endpoint

You need to configure an inference endpoint in Elasticsearch before using this tool:

```bash
PUT _inference/text_embedding/my-embedding-model
{
  "service": "elasticsearch",
  "service_settings": {
    "model_id": ".multilingual-e5-small",
    "num_allocations": 1,
    "num_threads": 1
  }
}
```

Or use a third-party service like OpenAI, Cohere, or HuggingFace:

```bash
PUT _inference/text_embedding/my-embedding-model
{
  "service": "openai",
  "service_settings": {
    "api_key": "your_openai_api_key",
    "model_id": "text-embedding-3-small"
  }
}
```

### Important Requirements

- The inference endpoint must output vectors with **384 dimensions** (or match your mapping's `dims` configuration)
- The `INFERENCE_ID` in your `.env` must match the inference endpoint ID you created
- The same inference model should be used for both indexing and searching

## Troubleshooting

### Connection Issues

- Verify Elasticsearch URL and API key are correct
- Check network connectivity to Elasticsearch instance
- Ensure API key has read permissions on the index

### No Results

- Check that documents exist in the index
- Lower the `min_score` parameter
- Verify the index name matches your `.env` configuration

### Vector Dimension Mismatch

- Ensure your inference endpoint outputs vectors matching the `dims` in your mapping (default: 384)
- Verify the inference endpoint is configured correctly: `GET _inference/text_embedding/my-embedding-model`
- Check that the same embedding model is used for both indexing and querying

### Inference API Issues

- Verify your Elasticsearch instance has the Inference API enabled
- Check that the `INFERENCE_ID` matches an existing inference endpoint
- Ensure the inference endpoint is deployed and running
- For Elasticsearch-hosted models, verify they are downloaded and allocated
- Check that `main_text` field uses a valid `inference_id` in the mapping

### Search Not Working

- Ensure Elasticsearch version is 8.0+ with Inference API support
- Check logs for inference-related errors
- Verify the inference endpoint can process text: `POST _inference/text_embedding/my-embedding-model { "input": "test" }`

## Development

### Project Structure

```
document_search_tool/
├── server.py           # Main MCP server implementation
├── requirements.txt    # Python dependencies
├── .env.example       # Example environment configuration
├── .env               # Your configuration (gitignored)
└── README.md          # This file
```

### Adding Features

The server is structured for easy extension:

- Modify `create_hybrid_search_query()` to adjust search behavior
- Update `format_search_results()` to change output formatting
- Add new tools by extending the `@app.list_tools()` and `@app.call_tool()` handlers

